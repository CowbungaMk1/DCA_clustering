% I highly recommend using JabRef as your reference manager.

@article{Elgamal1985,
  author   = {Elgamal, T.},
  journal  = {IEEE Transactions on Information Theory},
  title    = {A public key cryptosystem and a signature scheme based on discrete logarithms},
  year     = {1985},
  issn     = {1557-9654},
  month    = jul,
  number   = {4},
  pages    = {469--472},
  volume   = {31},
  doi      = {10.1109/TIT.1985.1057074},
  groups   = {Encryption},
  url      = {https://ieeexplore.ieee.org/document/1057074},
}

@article{jain_data_1999,
	title = {Data clustering: a review},
	volume = {31},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Data clustering},
	url = {https://dl.acm.org/doi/10.1145/331499.331504},
	doi = {10.1145/331499.331504},
	abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
	language = {en},
	number = {3},
	urldate = {2025-04-23},
	journal = {ACM Computing Surveys},
	author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
	month = sep,
	year = {1999},
	pages = {264--323},
	file = {Full Text PDF:files/848/Jain et al. - 1999 - Data clustering a review.pdf:application/pdf},
}

@article{piccialli_sos-sdp_2022,
	title = {{SOS}-{SDP}: an {Exact} {Solver} for {Minimum} {Sum}-of-{Squares} {Clustering}},
	volume = {34},
	issn = {1091-9856, 1526-5528},
	shorttitle = {{SOS}-{SDP}},
	url = {http://arxiv.org/abs/2104.11542},
	doi = {10.1287/ijoc.2022.1166},
	abstract = {The minimum sum-of-squares clustering problem (MSSC) consists of partitioning \$n\$ observations into \$k\$ clusters in order to minimize the sum of squared distances from the points to the centroid of their cluster. In this paper, we propose an exact algorithm for the MSSC problem based on the branch-and-bound technique. The lower bound is computed by using a cutting-plane procedure where valid inequalities are iteratively added to the Peng-Wei SDP relaxation. The upper bound is computed with the constrained version of k-means where the initial centroids are extracted from the solution of the SDP relaxation. In the branch-and-bound procedure, we incorporate instance-level must-link and cannot-link constraints to express knowledge about which data points should or should not be grouped together. We manage to reduce the size of the problem at each level preserving the structure of the SDP problem itself. The obtained results show that the approach allows to successfully solve for the first time real-world instances up to 4000 data points.},
	number = {4},
	urldate = {2025-04-24},
	journal = {INFORMS Journal on Computing},
	author = {Piccialli, Veronica and Sudoso, Antonio M. and Wiegele, Angelika},
	month = jul,
	year = {2022},
	note = {arXiv:2104.11542 [math]},
	keywords = {Mathematics - Optimization and Control},
	pages = {2144--2162},
	file = {Full Text PDF:files/871/Piccialli et al. - 2022 - SOS-SDP an Exact Solver for Minimum Sum-of-Square.pdf:application/pdf;Snapshot:files/870/2104.html:text/html},
}

@article{bagirov_nonsmooth_2016,
	title = {Nonsmooth {DC} programming approach to the minimum sum-of-squares clustering problems},
	volume = {53},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320315004318},
	doi = {10.1016/j.patcog.2015.11.011},
	abstract = {This paper introduces an algorithm for solving the minimum sum-of-squares clustering problems using their difference of convex representations. A non-smooth non-convex optimization formulation of the clustering problem is used to design the algorithm. Characterizations of critical points, stationary points in the sense of generalized gradients and inf-stationary points of the clustering problem are given. The proposed algorithm is tested and compared with other clustering algorithms using large real world data sets.},
	urldate = {2025-04-24},
	journal = {Pattern Recognition},
	author = {Bagirov, Adil M. and Taheri, Sona and Ugon, Julien},
	month = may,
	year = {2016},
	keywords = {Cluster analysis, Incremental clustering algorithms, Non-convex optimization, Non-smooth optimization},
	pages = {12--24},
	file = {Bagirov et al. - 2016 - Nonsmooth DC programming approach to the minimum s.pdf:files/877/Bagirov et al. - 2016 - Nonsmooth DC programming approach to the minimum s.pdf:application/pdf;ScienceDirect Snapshot:files/876/S0031320315004318.html:text/html},
}

@article{demyanov_method_2002,
	title = {A method of truncated codifferential with application to some problems of cluster analysis},
	volume = {23},
	issn = {1573-2916},
	url = {https://doi.org/10.1023/A:1014075113874},
	doi = {10.1023/A:1014075113874},
	abstract = {A method of truncated codifferential descent for minimizing continuously codifferentiable functions is suggested. The convergence of the method is studied. Results of numerical experiments are presented. Application of the suggested method for the solution of some problems of cluster analysis are discussed. In numerical experiments Wisconsin Diagnostic Breast Cancer database was used.},
	language = {en},
	number = {1},
	urldate = {2025-04-24},
	journal = {Journal of Global Optimization},
	author = {Demyanov, V.F. and Bagirov, A.M. and Rubinov, A.M.},
	month = may,
	year = {2002},
	keywords = {cluster analysis, codifferential, quasidifferential, subdifferential, truncated codifferential},
	pages = {63--80},
	file = {Full Text PDF:files/881/Demyanov et al. - 2002 - A method of truncated codifferential with applicat.pdf:application/pdf},
}

@article{an_new_2007,
	title = {A new efficient algorithm based on {DC} programming and {DCA} for clustering},
	volume = {37},
	issn = {1573-2916},
	url = {https://doi.org/10.1007/s10898-006-9066-4},
	doi = {10.1007/s10898-006-9066-4},
	abstract = {In this paper, a version of K-median problem, one of the most popular and best studied clustering measures, is discussed. The model using squared Euclidean distances terms to which the K-means algorithm has been successfully applied is considered. A fast and robust algorithm based on DC (Difference of Convex functions) programming and DC Algorithms (DCA) is investigated. Preliminary numerical solutions on real-world databases show the efficiency and the superiority of the appropriate DCA with respect to the standard K-means algorithm.},
	language = {en},
	number = {4},
	urldate = {2025-04-24},
	journal = {Journal of Global Optimization},
	author = {An, Le Thi Hoai and Belghiti, M. Tayeb and Tao, Pham Dinh},
	month = apr,
	year = {2007},
	keywords = {15A60, 65K05, 65K10, 90C06, 90C26, 90C90, Clustering, DC programming, DCA, K-means algorithm, K-median problem, Nonsmooth nonconvex programming},
	pages = {593--608},
	file = {Full Text PDF:files/884/An et al. - 2007 - A new efficient algorithm based on DC programming .pdf:application/pdf},
}

@article{artacho_boosted_nodate,
	title = {The {Boosted} {DC} {Algorithm} for nonsmooth functions},
	abstract = {The Boosted Diﬀerence of Convex functions Algorithm (BDCA) was recently proposed for minimizing smooth diﬀerence of convex (DC) functions. BDCA accelerates the convergence of the classical Diﬀerence of Convex functions Algorithm (DCA) thanks to an additional line search step. The purpose of this paper is twofold. Firstly, to show that this scheme can be generalized and successfully applied to certain types of nonsmooth DC functions, namely, those that can be expressed as the diﬀerence of a smooth function and a possibly nonsmooth one. Secondly, to show that there is complete freedom in the choice of the trial step size for the line search, which is something that can further improve its performance. We prove that any limit point of the BDCA iterative sequence is a critical point of the problem under consideration, and that the corresponding objective value is monotonically decreasing and convergent. The global convergence and convergent rate of the iterations are obtained under the Kurdyka–Lojasiewicz property. Applications and numerical experiments for two problems in data science are presented, demonstrating that BDCA outperforms DCA. Speciﬁcally, for the Minimum Sum-of-Squares Clustering problem, BDCA was on average sixteen times faster than DCA, and for the Multidimensional Scaling problem, BDCA was three times faster than DCA.},
	language = {en},
	author = {Artacho, Francisco J Aragón and Vuong, Phan T},
	file = {Artacho and Vuong - The Boosted DC Algorithm for nonsmooth functions.pdf:files/889/Artacho and Vuong - The Boosted DC Algorithm for nonsmooth functions.pdf:application/pdf},
}

@incollection{noauthor_minimum_nodate,
	title = {Minimum {Sum}-of-{Squares} {Clustering} by {DC} {Programming} and {DCA}},
}

@article{le_thi_exact_2012,
	title = {Exact penalty and error bounds in {DC} programming},
	volume = {52},
	issn = {1573-2916},
	url = {https://doi.org/10.1007/s10898-011-9765-3},
	doi = {10.1007/s10898-011-9765-3},
	abstract = {In the present paper, we are concerned with conditions ensuring the exact penalty for nonconvex programming. Firstly, we consider problems with concave objective and constraints. Secondly, we establish various results on error bounds for systems of DC inequalities and exact penalty, with/without error bounds, in DC programming. They permit to recast several class of difficult nonconvex programs into suitable DC programs to be tackled by the efficient DCA.},
	language = {en},
	number = {3},
	urldate = {2025-04-28},
	journal = {Journal of Global Optimization},
	author = {Le Thi, Hoai An and Pham Dinh, Tao and Ngai, Huynh Van},
	month = mar,
	year = {2012},
	keywords = {90C26, DC programming, DCA, 49J52, 90C20, 90C30, 90C46, Concave programming, Exact penalty, Local and global error bounds, Reformulation, Subdifferential},
	pages = {509--535},
	file = {Full Text PDF:files/919/Le Thi et al. - 2012 - Exact penalty and error bounds in DC programming.pdf:application/pdf},
}

@misc{Ordin_heuristic_2015,
	title = {A heuristic algorithm for solving the minimum sum-of-squares clustering problems},
	url = {https://www.researchgate.net/publication/272018375_A_heuristic_algorithm_for_solving_the_minimum_sum-of-squares_clustering_problems},
	abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
	language = {en},
	urldate = {2025-04-28},
	journal = {ResearchGate},
        author =  {Ordin, Burak and Bagirov, Adil M. },
        year = 2015
	file = {A heuristic algorithm for solving the minimum sum-.pdf:files/922/A heuristic algorithm for solving the minimum sum-.pdf:application/pdf;Snapshot:files/921/272018375_A_heuristic_algorithm_for_solving_the_minimum_sum-of-squares_clustering_problems.html:text/html},
}

@inproceedings{an_minimum_2009,
	address = {Berlin, Heidelberg},
	series = {{ICIC}'09},
	title = {Minimum sum-of-squares clustering by {DC} programming and {DCA}},
	isbn = {978-3-642-04019-1},
	abstract = {In this paper, we propose a new approach based on DC (Difference of Convex functions) programming and DCA (DC Algorithm) to perform clustering via minimum sum-of-squares Euclidean distance. The so called Minimum Sum-of-Squares Clustering (MSSC in short) is first formulated in the form of a hard combinatorial optimization problem. It is afterwards recast as a (continuous){\textbar} DC program with the help of exact penalty in DC programming. A DCA scheme is then investigated. The related DCA is original and very inexpensive because it amounts to computing, at each iteration, the projection of points onto a simplex and/or onto a ball, that all are given in the explicit form. Numerical results on real word data sets show the efficiency of DCA and its great superiority with respect to K-means, a standard method of clustering.},
	urldate = {2025-04-30},
	booktitle = {Proceedings of the {Intelligent} computing 5th international conference on {Emerging} intelligent computing technology and applications},
	publisher = {Springer-Verlag},
	author = {An, Le Thi Hoai and Tao, Pham Dinh},
	month = sep,
	year = {2009},
	pages = {327--340},
	file = {Minimum Sum-of-Squares Clustering by DC Programmin.pdf:files/896/Minimum Sum-of-Squares Clustering by DC Programmin.pdf:application/pdf},
}

@incollection{macqueen_methods_1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {5.1},
	url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992},
	urldate = {2025-05-01},
	booktitle = {Proceedings of the {Fifth} {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}, {Volume} 1: {Statistics}},
	publisher = {University of California Press},
	author = {MacQueen, J.},
	month = jan,
	year = {1967},
	pages = {281--298},
	file = {Full Text PDF:files/936/MacQueen - 1967 - Some methods for classification and analysis of mu.pdf:application/pdf},
}

@incollection{tao_algorithms_1986,
	series = {Fermat {Days} 85: {Mathematics} for {Optimization}},
	title = {Algorithms for {Solving} a {Class} of {Nonconvex} {Optimization} {Problems}. {Methods} of {Subgradients}},
	volume = {129},
	url = {https://www.sciencedirect.com/science/article/pii/S0304020808724022},
	abstract = {An algorithm does not make use of the analytical characterizations of optimal solutions that resort to the qualitative study of a class of problems. The first step consists of an effective realization of simple and finite algorithms for solving a problem. The chapter adapts the algorithms to solve the problems. The methods of subgradients are very easy to program and are actually the only ones which enables to treat practical problems of large size; their only drawback is that one cannot say in general, if the local maximums obtained are effectively global maximums, except if an initial vector x° is close enough to a global maximum. In the latter case, this is possible because of the duality in convex optimization used above for establishing the relations between the solutions of the primal and dual problems. Furthermore, this chapter also highlights duality schemes for problem (P1) and to methods of subgradients for solving this problem.},
	urldate = {2025-05-02},
	booktitle = {North-{Holland} {Mathematics} {Studies}},
	publisher = {North-Holland},
	author = {Tao, Pham Dinh and Souad, El Bernoussi},
	editor = {Hiriart-Urruty, J. -B.},
	month = jan,
	year = {1986},
	doi = {10.1016/S0304-0208(08)72402-2},
	pages = {249--271},
	file = {ScienceDirect Snapshot:files/940/S0304020808724022.html:text/html},
}

@article{tao_convex_1997,
	title = {{CONVEX} {ANALYSIS} {APPROACH} {TO} {D}. {C}. {PROGRAMMING}: {THEORY}, {ALGORITHMS} {AND} {APPLICATIONS}},
	abstract = {This paper is devoted to a thorough study on convex analysis approach to d.c. (diﬀerence of convex functions) programming and gives the State of the Art. Main results about d.c. duality, local and global optimalities in d.c. programming are presented. These materials constitute the basis of the DCA (d.c. algorithms). Its convergence properties have been tackled in detail, especially in d.c. polyhedral programming where it has ﬁnite convergence. Exact penalty, Lagrangian duality without gap, and regularization techniques have beeen studied to ﬁnd appropriate d.c. decompositions and to improve consequently the DCA. Finally we present the application of the DCA to solving a lot of important real-life d.c. programs.},
	language = {en},
	author = {Tao, Pham Dinh},
        year = {1997},
	file = {Tao - CONVEX ANALYSIS APPROACH TO D. C. PROGRAMMING THE.pdf:files/943/Tao - CONVEX ANALYSIS APPROACH TO D. C. PROGRAMMING THE.pdf:application/pdf},
}

@misc{noauthor_cluster_nodate,
	title = {Cluster {Analysis} of {Roll} {Call} {Votes} in {U}.{S}. {House}},
	url = {https://kaggle.com/code/aavigan/cluster-analysis-of-roll-call-votes-in-u-s-house},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from The 116th U.S. House of Representatives},
	language = {en},
	urldate = {2025-05-02},
	file = {Snapshot:files/946/cluster-analysis-of-roll-call-votes-in-u-s-house.html:text/html},
}
